# ğŸ“§ Spam Email Classification

This project implements an automatic email spam classification system using machine learning techniques and data balancing methods.

## ğŸ“Š Dataset

**Source:** [Spam Email Classification Dataset - Kaggle](https://www.kaggle.com/datasets/somesh24/spambase/data)

The dataset contains information about emails labeled as spam or not spam, with the following details:

* **Total instances:** 4,601 emails
* **Class distribution:**

  * Spam (1): 1,813 emails (39.40%)
  * Not spam (0): 2,788 emails (60.60%)

### ğŸ“‹ Attribute Description

The dataset includes 57 attributes plus the target variable:

* **48 continuous attributes \[0,100]:** Percentage frequency of specific words in the email

  * `word_freq_WORD = 100 * (number of times WORD appears) / total words`

* **6 continuous attributes \[0,100]:** Percentage frequency of specific characters

  * `char_freq_CHAR = 100 * (number of CHAR occurrences) / total characters`

* **3 attributes for capital letter sequences:**

  * `capital_run_length_average`: Average length of uninterrupted capital letter sequences
  * `capital_run_length_longest`: Length of the longest uninterrupted sequence of capital letters
  * `capital_run_length_total`: Total number of capital letters in the email

* **Target variable:** `spam` (0 = not spam, 1 = spam)

---

## ğŸ”§ Methodology

### ğŸ› ï¸ Data Preprocessing

The preprocessing pipeline includes:

**For numerical variables:**

* ğŸ”¢ Missing value imputation (mean)
* ğŸ”² Square root transformation
* ğŸ“ Standardization (StandardScaler)

**For categorical variables:**

* ğŸ”¡ Missing value imputation (mode)
* ğŸ·ï¸ One-hot encoding

### ğŸ¤– Models Evaluated

Multiple classification algorithms were tested:

| Model                                  | Accuracy   | Std. Deviation |
| -------------------------------------- | ---------- | -------------- |
| ğŸŒ³ **Random Forest**                   | **98.75%** | **0.50%**      |
| ğŸš€ **Gradient Boosting**               | **98.51%** | **0.50%**      |
| ğŸ¯ **SVC**                             | **97.47%** | **0.67%**      |
| ğŸ“ˆ **Logistic Regression**             | **97.09%** | **0.67%**      |
| ğŸ“Š **Linear Discriminant Analysis**    | **95.14%** | **1.13%**      |
| ğŸ“‰ **Quadratic Discriminant Analysis** | **95.03%** | **1.03%**      |
| ğŸ§  **Gaussian Naive Bayes**            | **93.10%** | **1.75%**      |

### âš–ï¸ Data Balancing Techniques

Several resampling techniques were explored:

#### ğŸ“‰ Undersampling

* **TomekLinks:** 98.7% (0.5%)
* **Edited Nearest Neighbours (ENN):** 98.7% (0.5%)
* **Repeated ENN:** 98.6% (0.5%)
* **One-Sided Selection:** 98.7% (0.5%)
* **Neighbourhood Cleaning Rule:** 98.7% (0.5%)

#### ğŸ“ˆ Oversampling

* **Random OverSampler:** 98.8% (0.5%)
* **SMOTE:** 98.7% (0.6%)
* **BorderlineSMOTE:** 98.8% (0.5%)
* **ADASYN:** 98.8% (0.5%)

---

## ğŸ† Final Model

### âš™ï¸ Optimal Configuration

**Algorithm:** ğŸŒ³ Calibrated Random Forest

* Estimators: 1,000 trees
* Class balancing: `class_weight="balanced"`
* Probability calibration: Isotonic Regression (CV=3)
* Balancing technique: BorderlineSMOTE

### ğŸ“Š Performance Metrics

| Metric                   | Value     |
| ------------------------ | --------- |
| ğŸ¯ **ROC-AUC**           | **0.982** |
| ğŸ“ **Average Precision** | **0.978** |
| ğŸ“ **PR-AUC**            | **0.979** |

---

## ğŸ’» Technologies Used

* ğŸ **Python 3.8+**
* ğŸ¤– **scikit-learn**: ML models and preprocessing
* âš–ï¸ **imbalanced-learn**: Balancing techniques
* ğŸ¼ **pandas**: Data manipulation
* ğŸ”¢ **numpy**: Numerical operations

---

## ğŸ“ˆ Conclusions

* ğŸŒ³ **Random Forest** was the most effective algorithm for this task.
* ğŸ“ˆ **Oversampling techniques** (especially BorderlineSMOTE) significantly improved performance.
* ğŸ¯ **Probability calibration** provided more reliable probability estimates.
* ğŸ† The final model achieved exceptional performance with **ROC-AUC of 0.982**.
